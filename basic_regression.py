# -*- coding: utf-8 -*-
"""Basic Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mh9DhzjEf4UddWPr7_wUJ6vNf2WPikjx
"""

!pip install seaborn

from __future__ import absolute_import, division, print_function, unicode_literals
import pathlib 

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

import tensorflow as tf 
from tensorflow import keras
from tensorflow.keras import layers
print(tf.__version__)

dataset_path = keras.utils.get_file("auto-mpg.data", "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data")
dataset_path

column_names =['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin']
raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = "?", comment ='\t',sep =' ', skipinitialspace = True )

dataset = raw_dataset.copy()
dataset

dataset.isna().sum #seeing which values have unwanted entries

dataset.dropna() #deleting all NA values

origin = dataset.pop('Origin')
dataset['USA'] = (origin ==1)*1.0
dataset['EUR'] = (origin ==1)*1.0
dataset['Japan'] = (origin ==1)*1.0
dataset.tail()

#splitting data into train data.
#neural network has to be divided into 2 types. Train and test.
train_dataset = dataset.sample(frac= 0.8, random_state = 0) #80 % data is going into the train data set.
test_dataset = dataset.drop(train_dataset.index) #20 percent data is going into the test data set.

#inspecting the data. #having the pairplot.

sns.pairplot(train_dataset[["MPG", "Cylinders", "Displacement", "Weight"]], diag_kind = "kde" )
plt.show()

#looking at the overall stats of the data set.
#mean std were already present in the data set.
#simply describing the type of data and displaying the type of data already present in the data set.
train_stats = train_dataset.describe()
train_stats.pop("MPG") #pop(MPG) is missing we are extracting MPG and not showing it. MPG is the target value and we have to calculate it. The other values have been used to model the network and using this model to calculate the MPG and we will check with the target value to check the function. 
train_stats = train_stats.transpose()
train_stats

#Split features from labels.

train_labels = train_dataset.pop('MPG')
test_labels = test_dataset.pop('MPG')

#NORMALIZING THE DATA. VERY IMPORTANT.
#values are fluctuating quite a lot. Removing various ranges involves normalization.

def norm(x): #defining normal function.
   return ( x - train_stats['mean'])/ train_stats['std']
normed_train_data = norm(test_dataset)
normed_test_data = norm(test_dataset)

#Building the model, seeing it and then fitting it into the network. 

def build_model():
  model = keras.Sequential([
     layers.Dense(64, activation = tf.nn.relu, input_shape = [len(train_dataset.keys())]),
     layers.Dense(64, activation = tf.nn.relu),
     layers.Dense(1)                    
  ])
  optimizer = tf.keras.optimizers.RMSprop(0.001)

  model.compile(loss = 'mean_squared_error', 
                optimizer = optimizer, 
                metrics = ['mean_absolute_error', 'mean_squared_error'])
  return model

model = build_model()

#inspect the model 
model.summary()

example_batch = normed_train_data[:10]
example_result = model.predict(example_batch)
example_result

#training the model
#display training progress by printing a single dor for each completed epoch
#training the model for 1000 epochs.
class PrintDot(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    if epoch % 100 == 0: print('')
    print('.', end = '')

EPOCHS = 1000
  
history = model.fit(
  normed_train_data, train_labels,
  epochs=EPOCHS, validation_split = 0.2, verbose = 0,
  callbacks=[PrintDot()])

#visualising the models training progress using the stats stored in the history object

hist = pd.DataFrame(history.history)
hist['epoch'] = (history.epoch)
hist.tail()

#plotting

#UPDATE: Unfortunately, the data set is all with NaN's/N/A's hence no graph is being made.

def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [MPG]')
  plt.plot(hist['epoch'], hist['mean_absolute_error'],
           label = 'Train Error')
  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],
           label = 'Val Error')
  plt.ylim([0, 5])
  plt.legend()

  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Square Error [$MPG^2$')
  plt.plot(hist['epoch'], hist['mean_squared_error'],
           label = 'Train Error')
  plt.plot(hist['epoch'], hist['val_mean_squared_error'],
           label = 'Va; Error')
  plt.ylim ([0, 20])
  plt.legend()
  plt.show()

plot_history(history)

model = build_model()

#The patience parameter is the amount of epochs to check for improvement
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10)

history = model.fit(normed_train_data, train_labels, epochs = EPOCHS,
                    validation_split = 0.2, verbose = 0, callbacks = [early_stop, PrintDot()])

plot_history(history) 

#unfortunately dataset isn't clean.

#Making predictions in ML

test_predictions = model.predict(normed_test_data).flatten()

plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values[MPG]')
plt.ylabel('Predictins[MPG]')
plt.axis('equal')
plt.axis('square')
plt.xlim([0,plt.xlim()[1]])
plt.ylim([0,plt.ylim()[1]])
_ = plt.plot([-100, 100], [-100, 100])

plt.show()

#unfortunately dataset is not clean anymore from the website.
#pictures have still been provided for reference. 
#plots are supposed to be scattered along the line.

error = test_predictions - test_labels
plt.hist(error, bins=25)
plt.xlabel('Prediction Error[MPG]')
_ = plt.ylabel("count")
plt.show()

#unfortunately, data set isn't clean anymore for some odd reason.

#not getting Gaussien Distribution as data is approx 400 rather than in millions.

#Neural Network has been complete using Python Programming